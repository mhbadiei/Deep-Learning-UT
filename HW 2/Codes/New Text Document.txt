"""
Implementation of multilayer perceptron network (MLP) with convolutional neural network (CNN) for classification applications
Authors : 
Abbas Badiei --------------------> mh.badiei@ut.ac.ir
saeed mohammadi dashtaki --------> saeedmohammadi.d@ut.ac.ir
Students at Tehran University, School of Electrical and Computer Engineering
"""
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
from keras.models import Sequential
from numpy.core.arrayprint import DatetimeFormat
from numpy.core.fromnumeric import shape
from sklearn.model_selection import train_test_split
from keras.layers.core import Dense
from keras.layers import Input,Conv2D, Flatten
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
import itertools
from sklearn.metrics import precision_recall_fscore_support
import time
from google.colab import files

class Classifier:
    def __init__(self):
        self.epochs = None
        self.test_size = None
        self.NoOfNeuronsPerLayer = []
        self.activationFunction = []
        self.loss = ''
        self.optimizer = ''
        self.metrics = ''
        self.batch_size = None
        self.validation_split = None
        self.data = []
        self.label = []
        self.xTrain = []
        self.xTest = []
        self.yTrain = []
        self.yTest = []
        self.model = None
        self.result = None
        self.cnnFlag = False

    def datasetRetrieval(self):
        (self.xTrain, self.yTrain), (self.xTest, self.yTest) = fashion_mnist.load_data()
        self.xTrain = np.expand_dims(self.xTrain , -1)
        self.yTrain = np.expand_dims(self.yTrain , -1)

    def normalization(self):
        self.xTrain = np.reshape(self.xTrain, (60000, 784)) 
        self.xTest = np.reshape(self.xTest, (10000, 784)) 
        normalizedData = MinMaxScaler()
        self.xTrain = normalizedData.fit_transform(self.xTrain)
        self.xTest = normalizedData.fit_transform(self.xTest)

    def standardization(self):
        srandardizedData = StandardScaler()
        self.xTrain = srandardizedData.fit_transform(self.xTrain)
        self.xTest = srandardizedData.fit_transform(self.xTest)
        self.xTrain = np.reshape(self.xTrain, (60000, 28, 28,1)) 
        self.xTest = np.reshape(self.xTest, (10000, 28, 28,1)) 


    def train(self,epochs = 15, NoOfNeuronsPerLayer = [400, 200], activationFunction = ['relu', 'relu', 'softmax'], loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'], batch_size = 256, validation_split = 0.2, cnnFlag = False):
        self.epochs = epochs
        self.NoOfNeuronsPerLayer = NoOfNeuronsPerLayer
        self.activationFunction = activationFunction
        self.loss = loss
        self.optimizer = optimizer
        self.metrics = metrics
        self.batch_size = batch_size
        self.validation_split = validation_split
        self.cnnFlag = cnnFlag
        self.train_()

    def train_(self):
        model = Sequential()
        model.add(Input(shape=self.xTrain[0].shape))
        if(self.cnnFlag):
          model.add(Conv2D(64, (4,4),strides = 2 , activation='relu'))
          model.add(Conv2D(64, (4,4),strides = 2 , activation='relu'))
          model.add(Conv2D(64, (4,4),strides = 2 , activation='relu'))
        model.add(Flatten())
        for count, No in enumerate(self.NoOfNeuronsPerLayer):
            layerName = "Hidden-Layer-No-" + str(count+1)
            model.add(Dense(No, activation=self.activationFunction[count], name=layerName))
        model.add(Dense(10, activation=self.activationFunction[-1], name="Output-Layer")) 
        model.summary()
        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)
        res = model.fit(self.xTrain, self.yTrain, epochs=self.epochs, validation_split=self.validation_split)
        self.result = res
        self.model = model

    def plotAccuracy(self):
        trainAcc = self.result.history['accuracy']
        evalAcc = self.result.history['val_accuracy']
        plt.figure()
        plt.plot(trainAcc,'red')
        plt.plot(evalAcc)
        plt.title('Accuracy of Model')
        plt.ylabel('ACCURACY')
        plt.xlabel('EPOCH')
        plt.legend(['Training Set', 'Validation Set'], loc='lower right')
        plt.grid()
        
    def plotLoss(self):
        trainLoss = self.result.history['loss']
        evalLoss = self.result.history['val_loss']
        plt.figure()
        plt.plot(trainLoss,'red')
        plt.plot(evalLoss)
        plt.title('Loss of Model')
        plt.ylabel('LOSS')
        plt.xlabel('EPOCH')
        plt.legend(['Training Set', 'Validation Set'], loc='upper right')
        plt.grid()
        
    def testModel(self):
        results = self.model.evaluate(self.xTest, self.yTest)
        print("Test Loss, Test Accuracy:", results)

    def showPlot(self):
        plt.show()

    def plotConfusionMatrix(self):
        cmap=plt.cm.Reds
        yPred = self.model.predict(self.xTest)
        predicted_categories = tf.argmax(yPred, axis=1)
        classes=[0,1,2,3,4,5,6,7,8,9]
        cm = confusion_matrix(self.yTest, predicted_categories)
        print("Confusion Matrix of Test Data:\n",cm)
        
        plt.figure(figsize = (4,4))
        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title('Confusion matrix')
        #plt.colorbar()
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes)
        plt.yticks(tick_marks, classes)
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, cm[i, j],
                    horizontalalignment="center",
                    color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')

if __name__ == "__main__":
    model = Classifier()
    model.datasetRetrieval()
    model.normalization()
    model.standardization()

    # this function just set the initial parameters of model in order to balance time complexity of algorithm for distinct inputs  
    model.train(epochs=1)

    """ Part A """
    # batchSizeList = [32, 64, 256]
    # elapsed = np.zeros((len(batchSizeList), 1))
    # rowLabel = [] 
    # for count, batchSize in enumerate(batchSizeList):
    #   t = time.time()
    #   model.train(batch_size = batchSize)
    #   elapsed[count] = time.time() - t        
    #   model.testModel()
    #   model.plotAccuracy()
    #   model.plotLoss()
    #   model.plotConfusionMatrix()
    #   rowLabel.append('batch: ' + str(batchSize))
    # plotTable(elapsed, rowLabel)

    """ Part B """
    activationFunctionList = ['tanh', 'relu', 'sigmoid']
    lossFuncList = ['mean_squared_error', 'sparse_categorical_crossentropy']
    elapsed = np.zeros((len(activationFunctionList)*len(lossFuncList), 1))
    rowLabel = [] 
    count= 0
    for loss in lossFuncList:
      for activationFunction in activationFunctionList:
        t = time.time()
        model.train(epochs=2, batch_size = 256, activationFunction = ['relu', 'relu'] + [activationFunction], loss=loss)
        elapsed[count] = time.time() - t        
        model.testModel()
        # model.plotAccuracy()
        # model.plotLoss()
        # model.plotConfusionMatrix()
        count+=1



    #for count, No in enumerate(NoOfNeuronsPerLayerList):
    #    t = time.time()
    #    model.train(NoOfNeuronsPerLayer = No)
    #    elapsed[0,count] = time.time() - t        
    #    model.testModel()
    #    model.plotAccuracy()
    #    model.plotLoss()
    #    model.plotConfusionMatrix()
    #plotTable(elapsed)

    """ Part D """
    #for count, secondActivationFunc in enumerate(secondActivationFuncList):
    #  model.train(batch_size = 64, NoOfNeuronsPerLayer = [15, 6], activationFunction = ['relu', secondActivationFunc, 'sigmoid'])
    #  model.testModel()
    #  model.plotAccuracy()
    #  model.plotLoss()
    #  model.plotConfusionMatrix()

    """" Part E """
    # for count, lossFunc in enumerate(lossFuncList):
    #     model.train(batch_size = 64, NoOfNeuronsPerLayer = [15, 6], activationFunction = ['relu', 'tanh', 'sigmoid'], loss=lossFunc)
    #     model.testModel()
    #     model.plotAccuracy()
    #     model.plotLoss()
    #     model.plotConfusionMatrix()

    """ Part F """
    # for count, optm in enumerate(optimizerList):
    #     model.train(batch_size = 64, NoOfNeuronsPerLayer = [15, 6], activationFunction = ['relu', 'tanh', 'sigmoid'], loss='binary_crossentropy', optimizer=optm)
    #     model.testModel()
    #     model.plotAccuracy()
    #     model.plotLoss()
    #     model.plotConfusionMatrix()

    """ Part G """
    # for count, No in enumerate(addLayersList):
    #     lengthOfHiddenLayer = len(No)
    #     NoOfReluActivationFunc =  ['relu']*(lengthOfHiddenLayer - 1)
    #     model.train(batch_size = 64, NoOfNeuronsPerLayer = No, activationFunction = NoOfReluActivationFunc + ['tanh', 'sigmoid'], loss='hinge', optimizer='adam')
    #     model.testModel()
    #     model.plotAccuracy()
    #     model.plotLoss()
    #     model.plotConfusionMatrix()
    #     model.precisionRecallFscorSupport()

    model.showPlot()

